{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1111676,"sourceType":"datasetVersion","datasetId":623289}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        (os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-19T13:38:05.891854Z","iopub.execute_input":"2024-07-19T13:38:05.892119Z","iopub.status.idle":"2024-07-19T13:38:11.906425Z","shell.execute_reply.started":"2024-07-19T13:38:05.892093Z","shell.execute_reply":"2024-07-19T13:38:11.905051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch import nn\nimport torchvision\ntorch.__version__, torchvision.__version__\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n# !nvidia-smi\nimport os\nimage_dir = r\"/kaggle/input/flickr8k/Images\"\ntext_dir = r\"/kaggle/input/flickr8k/captions.txt\"\ntext_dir\ndata = pd.read_csv(r\"/kaggle/input/flickr8k/captions.txt\", sep=',')\ndata.head()\n\nimage_arr = data.image.to_numpy()\ncaptions_arr = data.caption.to_numpy()\ncaptions_list = [i.lower().split(\" \") for i in captions_arr]\n# captions_list\n\nlen_caps = max([len(i) for i in captions_list])\nprint(len_caps)\nvocab_list = []\nfor i in captions_list:\n    vocab_list+=i\nx = np.unique(vocab_list)\nvocabulary = {word:idx for idx, word in enumerate(x, start=1)}\nvocabulary[\"<PAD>\"] = 0\na =len(vocabulary)\nvocabulary[\"<SOS>\"] = a\nvocabulary[\"<EOS>\"] = a + 1\nx.shape\n\nvoc_inv = {idx: word for word, idx in vocabulary.items()}\n\ndef encode(caption, maxLen:int=50):\n    caption = caption.split(\" \")\n    encoded_txt = np.array([vocabulary['<SOS>']] + [vocabulary[word] for word in caption if word in vocabulary])\n    x = maxLen - (len(encoded_txt) + 1)\n    padding = np.zeros(x)\n    eos = np.array([vocabulary[\"<EOS>\"]])\n    return np.hstack((encoded_txt, eos, padding))\n","metadata":{"execution":{"iopub.status.busy":"2024-07-19T13:38:11.908617Z","iopub.execute_input":"2024-07-19T13:38:11.909474Z","iopub.status.idle":"2024-07-19T13:38:17.316323Z","shell.execute_reply.started":"2024-07-19T13:38:11.909430Z","shell.execute_reply":"2024-07-19T13:38:17.315535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['encoded'] = data['caption'].apply(encode)\ndata['length'] = data['caption'].apply(lambda x: len(x.split(\" \")))\ndata['encodedlength'] = data['encoded'].apply(lambda x: len(x))\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-19T13:38:17.317457Z","iopub.execute_input":"2024-07-19T13:38:17.317745Z","iopub.status.idle":"2024-07-19T13:38:18.051147Z","shell.execute_reply.started":"2024-07-19T13:38:17.317721Z","shell.execute_reply":"2024-07-19T13:38:18.050294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\n# auto_transform = torchvision.transforms.Compose([\n#     torchvision.transforms.Resize((224, 224)),\n#     torchvision.transforms.ToTensor(),\n#     torchvision.transforms.Normalize([116.42541653, 113.22954264, 102.61758791], [14.8880556, 14.53207325, 16.18046796]),\n# ])\nfrom torchvision.models import resnet50, ResNet50_Weights\nweights = ResNet50_Weights.DEFAULT\nauto_transform = weights.transforms()\nclass ImageDataLoader(torch.utils.data.DataLoader):\n    def __init__(self, df, imgDir = image_dir, transforms=auto_transform ):\n        self.transform = transforms\n        self.imageDir = imgDir\n        self.image = df.image\n        self.captions = df.caption\n    \n    def __getitem__(self, idx):\n        path = os.path.join(self.imageDir, self.image[idx])\n        image = Image.open(path)\n        image = self.transform(image)\n        cap = encode(self.captions[idx])\n        return image, torch.Tensor(cap)\n    \n    def __len__(self):\n        return len(self.image)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T13:38:18.053346Z","iopub.execute_input":"2024-07-19T13:38:18.053634Z","iopub.status.idle":"2024-07-19T13:38:18.061124Z","shell.execute_reply.started":"2024-07-19T13:38:18.053610Z","shell.execute_reply":"2024-07-19T13:38:18.060297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_train_split = 0.87\nidx = int(test_train_split * len(data))\ntrain = data[:idx]\ntest = data[idx:]\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-19T13:38:18.062269Z","iopub.execute_input":"2024-07-19T13:38:18.062615Z","iopub.status.idle":"2024-07-19T13:38:18.085541Z","shell.execute_reply.started":"2024-07-19T13:38:18.062585Z","shell.execute_reply":"2024-07-19T13:38:18.084726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainDataset = ImageDataLoader(df = train)\ntestDataset = ImageDataLoader(df = test)\nDataset = ImageDataLoader(df = data)\ntrainDataset","metadata":{"execution":{"iopub.status.busy":"2024-07-19T13:38:18.086623Z","iopub.execute_input":"2024-07-19T13:38:18.086938Z","iopub.status.idle":"2024-07-19T13:38:18.095753Z","shell.execute_reply.started":"2024-07-19T13:38:18.086907Z","shell.execute_reply":"2024-07-19T13:38:18.094866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainDataloader = torch.utils.data.DataLoader(trainDataset, batch_size=32, shuffle=True)\ntestDataloader = torch.utils.data.DataLoader(testDataset, batch_size=32, shuffle=False)\ncompleteDataloader = torch.utils.data.DataLoader(Dataset, batch_size=32, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T13:38:18.096650Z","iopub.execute_input":"2024-07-19T13:38:18.096892Z","iopub.status.idle":"2024-07-19T13:38:18.105043Z","shell.execute_reply.started":"2024-07-19T13:38:18.096872Z","shell.execute_reply":"2024-07-19T13:38:18.104175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EncoderCNN(nn.Module):\n    def __init__(self, embed_size):\n        super(EncoderCNN, self).__init__()\n        resnet = models.resnet50(weights = models.ResNet50_Weights.DEFAULT)\n        modules = list(resnet.children())[:-1]\n        self.resnet = nn.Sequential(*modules)\n        for param in self.resnet.parameters():\n            param.requires_grad = False\n            \n        self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n        self.batch_norm = nn.BatchNorm1d(embed_size)\n        \n    def forward(self, img_tensors):\n        features = self.resnet(img_tensors)\n        features = features.view(features.shape[0], -1)\n        features = self.linear(features)\n        features = self.batch_norm(features)\n        return features","metadata":{"execution":{"iopub.status.busy":"2024-07-19T13:38:18.106203Z","iopub.execute_input":"2024-07-19T13:38:18.106988Z","iopub.status.idle":"2024-07-19T13:38:18.115324Z","shell.execute_reply.started":"2024-07-19T13:38:18.106958Z","shell.execute_reply":"2024-07-19T13:38:18.114491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DecoderRNN(nn.Module):\n    def __init__(self, embed_size, hidden_size, vocab_size, num_layers = 1):\n        super(DecoderRNN, self).__init__()\n        self.drop_prob = 0.3\n        self.num_layers = num_layers\n        self.hidden_size = hidden_size\n        self.embed = nn.Embedding(vocab_size, embed_size)\n        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first = True)\n        self.dropout = nn.Dropout(self.drop_prob)\n        self.linear = nn. Linear(hidden_size, vocab_size)\n        \n    def init_hidden(self, batch_size):\n        return (torch.zeros(self.num_layers, batch_size, self.hidden_size, device=device),\n                torch.zeros(self.num_layers, batch_size, self.hidden_size, device=device))\n\n    def init_hidden_eval(self):\n        return (torch.zeros(self.num_layers, self.hidden_size, device=device),\n                torch.zeros(self.num_layers, self.hidden_size, device=device))\n    \n    def forward(self, features, captions):\n        captions = captions.long()\n        captions = captions[:, :-1]\n        embeddings = self.embed(captions)\n        features = features.unsqueeze(1)\n        combined = torch.cat((features, embeddings), dim = 1)\n        self.batch_size = combined.shape[0]\n        self.hidden = self.init_hidden(self.batch_size)\n        hiddens, cells = self.lstm(combined, self.hidden)\n        output = self.linear(hiddens)\n        return output\n    \n    def generate(self, features):\n        \n        caption = []\n        hidden = self.init_hidden(features.shape[0])\n        features = features.unsqueeze(1)\n        x = 0\n        while x<51:\n            hiddens, hidden = self.lstm(features, hidden)\n            outputs = self.linear(hiddens)\n            outputs = outputs.squeeze(1)\n            \n            value, index = torch.max(outputs, dim = 1)\n            caption.append(voc_inv[index.item()])\n            \n            if voc_inv[index.item()] == '<EOS>':\n                break\n            x+=1\n                \n            features = self.embed(torch.tensor(index.item(), dtype = torch.long).to(device))\n            features = features.unsqueeze(0)\n            features = features.unsqueeze(0)\n            \n        return caption","metadata":{"execution":{"iopub.status.busy":"2024-07-19T13:38:18.116489Z","iopub.execute_input":"2024-07-19T13:38:18.116755Z","iopub.status.idle":"2024-07-19T13:38:18.130814Z","shell.execute_reply.started":"2024-07-19T13:38:18.116723Z","shell.execute_reply":"2024-07-19T13:38:18.130074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision import models\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(device)\nbatch_size = 32\nembed_size = 512\nvocab_size = len(vocabulary)\nhidden_size = 512\nnum_epochs = 3\n\nencoder = EncoderCNN(embed_size).to(device)\ndecoder = DecoderRNN(embed_size, hidden_size, vocab_size).to(device)\n\nloss_fn = nn.CrossEntropyLoss().to(device)\n\nparams = list(encoder.parameters()) + list(decoder.parameters())\n\noptimizer = torch.optim.Adam(params)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T13:38:18.133444Z","iopub.execute_input":"2024-07-19T13:38:18.133699Z","iopub.status.idle":"2024-07-19T13:38:19.982218Z","shell.execute_reply.started":"2024-07-19T13:38:18.133678Z","shell.execute_reply":"2024-07-19T13:38:19.981219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 1\nfor epoch in range(epochs):\n    trainLoss = 0\n    encoder.train()\n    decoder.train()\n    for batch, (image, caption) in enumerate(trainDataloader):\n        image = image.to(device)\n        caption = caption.to(device)\n        conv_res = encoder(image)\n        out = decoder(conv_res, caption)\n        out = out.contiguous().view(-1, vocab_size)\n        caption = caption.contiguous().view(-1).long()\n        loss = loss_fn(out, caption)\n        trainLoss+=loss.item()\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        if batch%250 == 0:\n            print(f\"Epoch:{epoch+1}| Loss:{loss}\")\n    trainLoss /= len(trainDataloader.dataset)\n    print(f\"Epoch:{epoch+1}|Training Loss:{trainLoss}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-19T13:38:19.983377Z","iopub.execute_input":"2024-07-19T13:38:19.983642Z","iopub.status.idle":"2024-07-19T13:44:28.683375Z","shell.execute_reply.started":"2024-07-19T13:38:19.983619Z","shell.execute_reply":"2024-07-19T13:44:28.682407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nrandomIdx = (random.randint(35196, 37254))\nx, y = test.image[randomIdx], test.caption[randomIdx]","metadata":{"execution":{"iopub.status.busy":"2024-07-19T14:01:47.662115Z","iopub.execute_input":"2024-07-19T14:01:47.662811Z","iopub.status.idle":"2024-07-19T14:01:47.667171Z","shell.execute_reply.started":"2024-07-19T14:01:47.662777Z","shell.execute_reply":"2024-07-19T14:01:47.666447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_img_file_name = '/kaggle/input/flickr8k/Images/' + x\n\nimg = Image.open(test_img_file_name)\n\nimg_tensor = auto_transform(img)\n\nimg_tensor = img_tensor.unsqueeze(0)\nimg_tensor = img_tensor.to(device)\n\nencoder.eval()\ndecoder.eval()\n\nfeature = encoder(img_tensor)\n# # print(feature.shape)\n\ncaption = decoder.generate(feature)\ncaption = caption[1:-1]\ncaption = \" \".join(caption)\ncaption\n\nplt.figure(figsize=(10, 7))\nplt.title(f\"Predicted Caption: {caption}\\nTrue Caption: {y}\")\nplt.imshow(img)\nplt.axis(False)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-19T14:01:49.055094Z","iopub.execute_input":"2024-07-19T14:01:49.055937Z","iopub.status.idle":"2024-07-19T14:01:49.598246Z","shell.execute_reply.started":"2024-07-19T14:01:49.055901Z","shell.execute_reply":"2024-07-19T14:01:49.597322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}